from sklearn.metrics import (
    precision_score, recall_score, f1_score,
    roc_auc_score, average_precision_score, 
    brier_score_loss, classification_report, 
    confusion_matrix, precision_recall_curve, roc_curve
)
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

def evaluate_model(model, X_val, y_val, model_name="Model", threshold=0.5):
    results = {}

    # Predict probabilities
    y_proba = model.predict_proba(X_val)[:, 1]
    y_pred = (y_proba >= threshold).astype(int)

    # Metrics
    precision = precision_score(y_val, y_pred)
    recall = recall_score(y_val, y_pred)
    f1 = f1_score(y_val, y_pred)
    roc_auc = roc_auc_score(y_val, y_proba)
    pr_auc = average_precision_score(y_val, y_proba)
    brier_score = brier_score_loss(y_val, y_proba)
    accuracy = (y_val == y_pred).mean()
    cm = confusion_matrix(y_val, y_pred)

    # Store results
    results.update({
        'model_name': model_name,
        'precision': precision,
        'recall': recall,
        'f1_score': f1,
        'roc_auc': roc_auc,
        'pr_auc': pr_auc,
        'brier_score_loss': brier_score,
        'accuracy': accuracy,
        'confusion_matrix': cm.tolist(),
    })

    # Print metrics
    print(f"\n----- {model_name} Evaluation -----")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1 Score: {f1:.4f}")
    print(f"ROC AUC: {roc_auc:.4f}")
    print(f"PR AUC: {pr_auc:.4f}")
    print(f"Brier Score Loss: {brier_score:.4f}")
    print(f"Accuracy: {accuracy * 100:.2f}%")
    print(classification_report(y_val, y_pred))

    # Plot all charts in one figure
    fig, axs = plt.subplots(1, 3, figsize=(20, 6))

    # Subplot 1: Confusion Matrix
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axs[0])
    axs[0].set_title(f'Confusion Matrix ({model_name})')
    axs[0].set_xlabel('Predicted Label')
    axs[0].set_ylabel('True Label')

    # Subplot 2: ROC Curve
    fpr, tpr, _ = roc_curve(y_val, y_proba)
    axs[1].plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}')
    axs[1].plot([0, 1], [0, 1], 'k--')
    axs[1].set_title('ROC Curve')
    axs[1].set_xlabel('False Positive Rate')
    axs[1].set_ylabel('True Positive Rate')
    axs[1].legend()

    # Subplot 3: Precision-Recall Curve
    precision_vals, recall_vals, _ = precision_recall_curve(y_val, y_proba)
    axs[2].plot(recall_vals, precision_vals, label=f'AUC = {pr_auc:.2f}')
    axs[2].set_title('Precision-Recall Curve')
    axs[2].set_xlabel('Recall')
    axs[2].set_ylabel('Precision')
    axs[2].legend()

    plt.tight_layout()
    plt.show()

    return results
