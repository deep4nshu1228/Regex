from sklearn.metrics import (
    precision_score, recall_score, f1_score,
    roc_auc_score, average_precision_score, 
    brier_score_loss, classification_report, 
    confusion_matrix, precision_recall_curve, roc_curve
)
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

def evaluate_model(model, X_val, y_val, X_test=None, y_test=None, model_name="Model", threshold=0.5):
    def compute_metrics(y_true, y_proba, dataset_name="Validation"):
        y_pred = (y_proba >= threshold).astype(int)

        precision = precision_score(y_true, y_pred)
        recall = recall_score(y_true, y_pred)
        f1 = f1_score(y_true, y_pred)
        roc_auc = roc_auc_score(y_true, y_proba)
        pr_auc = average_precision_score(y_true, y_proba)
        brier = brier_score_loss(y_true, y_proba)
        acc = (y_true == y_pred).mean()
        cm = confusion_matrix(y_true, y_pred)

        print(f"\n----- {model_name} - {dataset_name} Set -----")
        print(f"Precision: {precision:.4f}")
        print(f"Recall: {recall:.4f}")
        print(f"F1 Score: {f1:.4f}")
        print(f"ROC AUC: {roc_auc:.4f}")
        print(f"PR AUC: {pr_auc:.4f}")
        print(f"Brier Score Loss: {brier:.4f}")
        print(f"Accuracy: {acc * 100:.2f}%")
        print(classification_report(y_true, y_pred))

        return {
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'roc_auc': roc_auc,
            'pr_auc': pr_auc,
            'brier_score_loss': brier,
            'accuracy': acc,
            'confusion_matrix': cm,
            'y_proba': y_proba,
            'y_pred': y_pred,
        }

    results = {'model_name': model_name}

    # Validation metrics
    y_val_proba = model.predict_proba(X_val)[:, 1]
    val_results = compute_metrics(y_val, y_val_proba, dataset_name="Validation")
    results.update({f"val_{k}": v for k, v in val_results.items() if k != 'y_proba' and k != 'y_pred'})

    # Test metrics if provided
    if X_test is not None and y_test is not None:
        y_test_proba = model.predict_proba(X_test)[:, 1]
        test_results = compute_metrics(y_test, y_test_proba, dataset_name="Test")
        results.update({f"test_{k}": v for k, v in test_results.items() if k != 'y_proba' and k != 'y_pred'})

    # Plot both validation and test in one figure if test data exists
    n_plots = 6 if X_test is not None else 3
    fig, axs = plt.subplots(1, n_plots, figsize=(6 * n_plots, 6))

    def plot_metrics(ax_idx, y_true, y_pred, y_proba, label):
        # Confusion Matrix
        sns.heatmap(confusion_matrix(y_true, y_pred), annot=True, fmt='d', cmap='Blues', ax=axs[ax_idx])
        axs[ax_idx].set_title(f'Confusion Matrix ({label})')
        axs[ax_idx].set_xlabel('Predicted')
        axs[ax_idx].set_ylabel('True')

        # ROC Curve
        fpr, tpr, _ = roc_curve(y_true, y_proba)
        axs[ax_idx + 1].plot(fpr, tpr, label=f'AUC = {roc_auc_score(y_true, y_proba):.2f}')
        axs[ax_idx + 1].plot([0, 1], [0, 1], 'k--')
        axs[ax_idx + 1].set_title(f'ROC Curve ({label})')
        axs[ax_idx + 1].set_xlabel('FPR')
        axs[ax_idx + 1].set_ylabel('TPR')
        axs[ax_idx + 1].legend()

        # PR Curve
        precision_vals, recall_vals, _ = precision_recall_curve(y_true, y_proba)
        axs[ax_idx + 2].plot(recall_vals, precision_vals, label=f'AUC = {average_precision_score(y_true, y_proba):.2f}')
        axs[ax_idx + 2].set_title(f'PR Curve ({label})')
        axs[ax_idx + 2].set_xlabel('Recall')
        axs[ax_idx + 2].set_ylabel('Precision')
        axs[ax_idx + 2].legend()

    # Plot for validation
    plot_metrics(0, y_val, val_results['y_pred'], y_val_proba, "Validation")

    # Plot for test if available
    if X_test is not None and y_test is not None:
        plot_metrics(3, y_test, test_results['y_pred'], y_test_proba, "Test")

    plt.tight_layout()
    plt.show()

    return results
